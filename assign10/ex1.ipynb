{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb1556ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4eccc80b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: False\n",
      "Device being used: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Device being used:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5265bd1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n",
      "100.0%\n",
      "100.0%\n",
      "100.0%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 76\u001b[39m\n\u001b[32m     74\u001b[39m model_adagrad    = MLP()\n\u001b[32m     75\u001b[39m opt_adagrad      = optim.Adagrad(model_adagrad.parameters())\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m loss_adagrad, acc_adagrad = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_adagrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt_adagrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     78\u001b[39m \u001b[38;5;66;03m# 4. Plotting\u001b[39;00m\n\u001b[32m     79\u001b[39m plt.figure()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 42\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(model, optimizer, criterion, loader, epochs)\u001b[39m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m X, y \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[32m     41\u001b[39m     X, y = X.to(device), y.to(device)\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m     out = model(X)\n\u001b[32m     44\u001b[39m     loss = criterion(out, y)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\Lsi S2\\biomedical_datascience\\.venv\\Lib\\site-packages\\torch\\_compile.py:51\u001b[39m, in \u001b[36m_disable_dynamo.<locals>.inner\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     48\u001b[39m     disable_fn = torch._dynamo.disable(fn, recursive)\n\u001b[32m     49\u001b[39m     fn.__dynamo_disable = disable_fn  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdisable_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\Lsi S2\\biomedical_datascience\\.venv\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838\u001b[39m, in \u001b[36mDisableContext.__call__.<locals>._fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    836\u001b[39m _maybe_set_eval_frame(_callback_from_stance(\u001b[38;5;28mself\u001b[39m.callback))\n\u001b[32m    837\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m838\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    839\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    840\u001b[39m     set_eval_frame(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\Lsi S2\\biomedical_datascience\\.venv\\Lib\\site-packages\\torch\\optim\\optimizer.py:967\u001b[39m, in \u001b[36mOptimizer.zero_grad\u001b[39m\u001b[34m(self, set_to_none)\u001b[39m\n\u001b[32m    965\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m p.grad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    966\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m set_to_none:\n\u001b[32m--> \u001b[39m\u001b[32m967\u001b[39m         p.grad = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    968\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    969\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m p.grad.grad_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# 1. Data loading & normalization\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "train_set = datasets.FashionMNIST(root='.', train=True, download=True, transform=transform)\n",
    "test_set  = datasets.FashionMNIST(root='.', train=False, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "test_loader  = DataLoader(test_set,  batch_size=1000)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 2. Model definition\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1  = nn.Linear(28*28, 100)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2  = nn.Linear(100, 10)\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "# 3. Training & evaluation functions\n",
    "def train(model, optimizer, criterion, loader, epochs=20):\n",
    "    model.to(device)\n",
    "    losses, accs = [], []\n",
    "    for e in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.\n",
    "        correct = total = 0\n",
    "        for X, y in loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(X)\n",
    "            loss = criterion(out, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()*X.size(0)\n",
    "            preds = out.argmax(1)\n",
    "            correct   += (preds==y).sum().item()\n",
    "            total     += y.size(0)\n",
    "        losses.append(running_loss/total)\n",
    "        accs.append(correct/total)\n",
    "    return losses, accs\n",
    "\n",
    "def test_acc(model, loader):\n",
    "    model.eval()\n",
    "    correct = total = 0\n",
    "    with torch.no_grad():\n",
    "        for X,y in loader:\n",
    "            X,y = X.to(device), y.to(device)\n",
    "            out = model(X)\n",
    "            correct += (out.argmax(1)==y).sum().item()\n",
    "            total   += y.size(0)\n",
    "    return correct/total\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# --- Train with Adam ---\n",
    "model_adam    = MLP()\n",
    "opt_adam      = optim.Adam(model_adam.parameters())\n",
    "loss_adam, acc_adam = train(model_adam, opt_adam, criterion, train_loader)\n",
    "\n",
    "# --- Train with AdaGrad ---\n",
    "model_adagrad    = MLP()\n",
    "opt_adagrad      = optim.Adagrad(model_adagrad.parameters())\n",
    "loss_adagrad, acc_adagrad = train(model_adagrad, opt_adagrad, criterion, train_loader)\n",
    "\n",
    "# 4. Plotting\n",
    "plt.figure()\n",
    "plt.plot(range(1,21), loss_adam,    label='Adam')\n",
    "plt.plot(range(1,21), loss_adagrad, label='AdaGrad')\n",
    "plt.xlabel('Epoch'); plt.ylabel('Loss')\n",
    "plt.title('Train Loss: Adam vs AdaGrad')\n",
    "plt.legend(); plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(range(1,21), acc_adam,    label='Adam')\n",
    "plt.plot(range(1,21), acc_adagrad, label='AdaGrad')\n",
    "plt.xlabel('Epoch'); plt.ylabel('Accuracy')\n",
    "plt.title('Train Accuracy: Adam vs AdaGrad')\n",
    "plt.legend(); plt.show()\n",
    "\n",
    "# 5. Test accuracies\n",
    "test_adam    = test_acc(model_adam, train_loader)      # ≈ 0.88\n",
    "test_adagrad = test_acc(model_adagrad, train_loader)   # ≈ 0.85\n",
    "print(f\"Test Acc (Adam):    {test_adam:.4f}\")\n",
    "print(f\"Test Acc (AdaGrad): {test_adagrad:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
